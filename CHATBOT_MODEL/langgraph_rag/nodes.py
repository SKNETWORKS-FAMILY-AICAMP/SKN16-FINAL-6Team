"""
LangGraph RAG ë…¸ë“œ í•¨ìˆ˜

ì´ ëª¨ë“ˆì€ LangGraph ì›Œí¬í”Œë¡œìš°ì˜ ê° ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ê° ë…¸ë“œëŠ” RAGStateë¥¼ ì…ë ¥ë°›ì•„ ìˆ˜ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

ì£¼ìš” ë…¸ë“œ:
- query_router: ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…
- hybrid_retrieve: Hybrid Search (Dense + Sparse + RRF)
- rerank_stage1: 1ì°¨ Reranking (BGE-reranker-v2-m3)
- rerank_stage2: 2ì°¨ Reranking (BGE-reranker-large)
- grade_documents: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
- transform_query: ì¿¼ë¦¬ ì¬ì‘ì„±
- generate: ë‹µë³€ ìƒì„±
- hallucination_check: í™˜ê° ê²€ì¦
- answer_grading: ë‹µë³€ í’ˆì§ˆ í‰ê°€
- web_search: ì›¹ ê²€ìƒ‰ fallback
"""

import logging
import re
import time
from typing import Dict, List, Tuple

import chromadb
from FlagEmbedding import BGEM3FlagModel, FlagReranker
from openai import AsyncOpenAI, OpenAI

# ### ìˆ˜ì • ì‹œì‘ ###
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
# ### ìˆ˜ì • ì™„ë£Œ ###

from .config import get_config
from .state import (
    RAGState,
    add_to_history,
    # ### ìˆ˜ì • ì‹œì‘ ###
    IntentClassification,
    IntentType,
    DocumentRelevance,
    RelevanceType,
    RewrittenQuery,
    QueryRewriteAction,
    HallucinationGrade,
    HallucinationType,
    UsefulnessGrade,
    UsefulnessType,
    # ### ìˆ˜ì • ì™„ë£Œ ###
)
from .tools import get_web_search_tool, get_rag_tools  # ### ìˆ˜ì •: get_rag_tools ì¶”ê°€ ###

logger = logging.getLogger(__name__)


def _increment_retry_count(state: RAGState) -> None:
    """retry_countëŠ” ë¼ìš°íŒ… í•¨ìˆ˜ ëŒ€ì‹  ì‹¤ì œ ì¬ì‹œë„ ë…¸ë“œì—ì„œë§Œ ì¦ê°€ì‹œí‚¨ë‹¤."""
    config = get_config()
    state["retry_count"] = min(state["retry_count"] + 1, config.max_retries)


# ========== ì „ì—­ ë¦¬ì†ŒìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´) ==========

class RAGResources:
    """
    RAG ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ (ì‹±ê¸€í†¤)

    LangGraph ë…¸ë“œë“¤ì´ ê³µìœ í•˜ëŠ” ë¦¬ì†ŒìŠ¤:
    - ì„ë² ë”© ëª¨ë¸
    - Reranker ëª¨ë¸ë“¤
    - ChromaDB ì»¬ë ‰ì…˜
    - LLM í´ë¼ì´ì–¸íŠ¸
    """

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)"""
        if self._initialized:
            return

        logger.info("Initializing RAG resources...")
        config = get_config()

        # ì„ë² ë”© ëª¨ë¸
        logger.info(f"Loading embedding model: {config.embedding_model}")
        self.embedding_model = BGEM3FlagModel(
            config.embedding_model,
            use_fp16=True,
            device=config.embedding_device,
        )
        self.embedding_batch_size = config.embedding_batch_size

        # Reranker Stage 1
        logger.info(f"Loading reranker stage 1: {config.reranker_stage1_model}")
        self.reranker_stage1 = FlagReranker(
            config.reranker_stage1_model,
            use_fp16=True,
            device=config.reranker_stage1_device,
        )

        # Reranker Stage 2
        logger.info(f"Loading reranker stage 2: {config.reranker_stage2_model}")
        self.reranker_stage2 = FlagReranker(
            config.reranker_stage2_model,
            use_fp16=True,
            device=config.reranker_stage2_device,
        )

        # ChromaDB
        logger.info(f"Connecting to ChromaDB at {config.chroma_db_path}")
        client = chromadb.PersistentClient(path=str(config.chroma_db_path))
        self.collection = client.get_collection("rag_chunks")
        logger.info(f"Collection loaded: {self.collection.count()} documents")

        # LLM í´ë¼ì´ì–¸íŠ¸ (ë™ê¸°/ë¹„ë™ê¸°)
        self.llm_client = OpenAI()
        self.async_llm_client = AsyncOpenAI()

        # ### ìˆ˜ì • ì‹œì‘ ###
        # LangChain LLM í´ë¼ì´ì–¸íŠ¸ (structured output / bind_tools ìš©)
        self.langchain_llm = ChatOpenAI(
            model=config.llm_model,
            temperature=config.llm_temperature,
        )
        self.langchain_llm_fast = ChatOpenAI(
            model=config.context_quality_model,
            temperature=0,
        )
        # ### ìˆ˜ì • ì™„ë£Œ ###

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt_path = config.artifacts_dir.parent / config.config["llm"]["system_prompt_path"]
        self.system_prompt = (
            system_prompt_path.read_text(encoding="utf-8")
            if system_prompt_path.exists()
            else ""
        )

        # ì›¹ ê²€ìƒ‰ ë„êµ¬
        self.web_search_tool = get_web_search_tool()

        self._initialized = True
        logger.info("âœ“ RAG resources initialized")


def get_resources() -> RAGResources:
    """ì „ì—­ RAG ë¦¬ì†ŒìŠ¤ ë°˜í™˜"""
    return RAGResources()

# ========== ë…¸ë“œ 0: Intent Classifier ==========


# ### ìˆ˜ì • ì‹œì‘ ###
def intent_classifier_node(state: RAGState) -> RAGState:
    """
    ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•´ in_scopeê°€ ì•„ë‹ˆë©´ ì´ˆê¸°ì— ì¢…ë£Œì‹œí‚¨ë‹¤.
    with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜.

    Categories:
    - IN_SCOPE: ê°œë°œ/í”„ë¡œê·¸ë˜ë°/í•™ìŠµ ê´€ë ¨
    - GREETING: ì¸ì‚¬/ê°ì‚¬ ë“±
    - CHITCHAT: ì¡ë‹´/ìš”ì²­(ì•„ì´ìŠ¤í¬ë¦¼ ì‚¬ì¤˜ ë“±)
    - NONSENSICAL: ë¬´ì˜ë¯¸/ìŠ¤íŒ¸
    """
    logger.info("[Intent] ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ ì‹œì‘")
    resources = get_resources()

    question = state["question"]
    intent = "unknown"

    system_prompt = """ë‹¹ì‹ ì€ ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ë¶„ë¥˜ ê¸°ì¤€:
- IN_SCOPE: ê°œë°œ, í”„ë¡œê·¸ë˜ë°, ì†Œí”„íŠ¸ì›¨ì–´ í•™ìŠµ/ë””ë²„ê¹…/ë„êµ¬ ì‚¬ìš© ê´€ë ¨
- GREETING: ì¸ì‚¬, ê°ì‚¬, ì•ˆë¶€
- CHITCHAT: ì¡ë‹´/ì‚¬ì ìš”ì²­ (ì˜ˆ: ì•„ì´ìŠ¤í¬ë¦¼ ì‚¬ì¤˜, ë…¸ë˜ ì¶”ì²œ)
- NONSENSICAL: ë¬´ì˜ë¯¸/ìŠ¤íŒ¸/ì˜ë¯¸ ì—†ëŠ” ì…ë ¥"""

    user_prompt = f"ì§ˆë¬¸: {question}"

    try:
        # with_structured_outputìœ¼ë¡œ IntentClassification Pydantic ëª¨ë¸ ê°•ì œ
        structured_llm = resources.langchain_llm_fast.with_structured_output(
            IntentClassification,
            method="function_calling",
        )
        result: IntentClassification = structured_llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ])

        intent = result.intent.value
        logger.info(f"[Intent] ë¶„ë¥˜ ê²°ê³¼: {intent}, ê·¼ê±°: {result.reasoning}")

    except Exception as e:
        logger.warning(f"[Intent] ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ in_scopeë¡œ ì²˜ë¦¬")
        intent = "in_scope"

    state["intent"] = intent

    # in_scopeê°€ ì•„ë‹ˆë©´ ë°”ë¡œ ì§§ì€ ë©”ì‹œì§€ í›„ ì¢…ë£Œ
    if intent != "in_scope":
        reply_map = {
            "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê°œë°œÂ·í•™ìŠµ ë„ìš°ë¯¸ì˜ˆìš”. ê¶ê¸ˆí•œ ê°œë°œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ì„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”.",
            "chitchat": "ì €ëŠ” ê°œë°œÂ·í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ê³  ìˆì–´ìš”. ì½”ë“œë‚˜ ì—ëŸ¬, í•™ìŠµ ì£¼ì œë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "nonsensical": "ì§€ê¸ˆ ì…ë ¥ìœ¼ë¡œëŠ” ë„ì›€ì„ ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”. ê°œë°œ/í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”.",
        }
        state["generation"] = reply_map.get(
            intent,
            "ê°œë°œÂ·í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ì„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ê²Œìš”.",
        )

    return add_to_history(state, "intent_classifier")
# ### ìˆ˜ì • ì™„ë£Œ ###


# ========== ë…¸ë“œ 1: Query Router ==========

def query_router_node(state: RAGState) -> RAGState:
    """
    ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ… ê²°ì •

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: ë¼ìš°íŒ… ê²°ì •ì´ ì¶”ê°€ëœ ìƒíƒœ

    ë¼ìš°íŒ… ì „ëµ:
    - "vectorstore": ë²¡í„° ê²€ìƒ‰ (ê¸°ë³¸)
    - "websearch": ì›¹ ê²€ìƒ‰ (ìµœì‹  ì •ë³´ í•„ìš”)
    - "direct": LLMë§Œ ì‚¬ìš© (ê²€ìƒ‰ ë¶ˆí•„ìš”)

    í˜„ì¬ êµ¬í˜„: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
    í–¥í›„ ê°œì„ : LLM ê¸°ë°˜ ë¶„ë¥˜
    """
    logger.info(f"[QueryRouter] ì§ˆë¬¸ ë¶„ì„: {state['question'][:100]}")

    question = state["question"].lower()

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
    # TODO: LLM ê¸°ë°˜ ë¶„ë¥˜ë¡œ ê°œì„ 
    if any(
        keyword in question
        for keyword in ["ìµœê·¼", "í˜„ì¬", "2024", "2025", "ë‰´ìŠ¤", "íŠ¸ë Œë“œ"]
    ):
        route = "websearch"
        logger.info("[QueryRouter] â†’ ì›¹ ê²€ìƒ‰ (ìµœì‹  ì •ë³´)")
    elif any(
        keyword in question
        for keyword in ["ì•ˆë…•", "hello", "hi", "ê°ì‚¬", "ê³ ë§ˆì›Œ"]
    ):
        route = "direct"
        logger.info("[QueryRouter] â†’ ì§ì ‘ ë‹µë³€ (ê²€ìƒ‰ ë¶ˆí•„ìš”)")
    else:
        route = "vectorstore"
        logger.info("[QueryRouter] â†’ ë²¡í„° ê²€ìƒ‰ (ê¸°ë³¸)")

    state["route"] = route
    return add_to_history(state, "query_router")


# ========== ë…¸ë“œ 2: Hybrid Retrieve ==========

def hybrid_retrieve_node(state: RAGState) -> RAGState:
    """
    Hybrid Search (Dense + Sparse + RRF Fusion)

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ê²€ìƒ‰ ë‹¨ê³„:
    1. ì¿¼ë¦¬ ì¸ì½”ë”© (Dense + Sparse)
    2. Dense ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)
    3. Sparse ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)
    4. RRF Fusion (ë‘ ê²°ê³¼ ê²°í•©)
    """
    logger.info("[HybridRetrieve] ê²€ìƒ‰ ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]

    # Step 1: ì¿¼ë¦¬ ì¸ì½”ë”©
    query_encoding = resources.embedding_model.encode(
        [question],
        batch_size=1,
        max_length=1024,
        return_dense=True,
        return_sparse=True,
        return_colbert_vecs=False,
    )
    query_dense = query_encoding["dense_vecs"][0].tolist()
    query_sparse = query_encoding["lexical_weights"][0]

    # Step 2: Dense ê²€ìƒ‰
    dense_top_k = config.hybrid_dense_top_k
    dense_results = resources.collection.query(
        query_embeddings=[query_dense],
        n_results=dense_top_k,
        include=["documents", "metadatas"],
    )
    dense_docs = dense_results["documents"][0]
    dense_ids = dense_results["ids"][0]
    dense_metas = dense_results["metadatas"][0]

    if not dense_docs:
        logger.warning("[HybridRetrieve] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        state["documents"] = []
        state["metadatas"] = []
        return add_to_history(state, "hybrid_retrieve")

    # Step 3: Sparse ê²€ìƒ‰
    sparse_top_k = config.hybrid_sparse_top_k
    dense_scored = [
        (dense_ids[i], dense_docs[i], 1.0 / (i + 1)) for i in range(len(dense_docs))
    ]
    sparse_scored = _sparse_search(
        resources, query_sparse, dense_docs, dense_ids, top_k=sparse_top_k
    )

    # Step 4: RRF Fusion
    rrf_k = config.config["retrieval"]["rrf_k"]
    fused_docs = _reciprocal_rank_fusion(dense_scored, sparse_scored, k=rrf_k)

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘ (O(1) ì¡°íšŒ)
    doc_to_meta = {}
    for i, doc in enumerate(dense_docs):
        meta = dense_metas[i].copy()
        meta["chunk_id"] = dense_ids[i]
        doc_to_meta[doc] = meta

    fused_metadatas = [
        doc_to_meta.get(doc, {"domain": "unknown", "chunk_id": "unknown"})
        for doc in fused_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[HybridRetrieve] {len(fused_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}s)"
    )

    state["documents"] = fused_docs
    state["metadatas"] = fused_metadatas
    return add_to_history(state, "hybrid_retrieve")


def _sparse_search(
    resources: RAGResources,
    query_sparse_vector: Dict,
    documents: List[str],
    doc_ids: List[str],
    top_k: int = 50,
) -> List[Tuple[str, str, float]]:
    """
    Sparse ê²€ìƒ‰ (BGE-M3 lexical weights ì‚¬ìš©)

    Args:
        resources: RAG ë¦¬ì†ŒìŠ¤
        query_sparse_vector: ì¿¼ë¦¬ sparse vector
        documents: í›„ë³´ ë¬¸ì„œë“¤
        doc_ids: ë¬¸ì„œ IDë“¤
        top_k: ìƒìœ„ kê°œ ë°˜í™˜

    Returns:
        List[Tuple[str, str, float]]: (doc_id, doc_text, score)
    """
    # ë¬¸ì„œ ì¸ì½”ë”© (sparseë§Œ)
    doc_encodings = resources.embedding_model.encode(
        documents,
        batch_size=resources.embedding_batch_size,
        max_length=1024,
        return_dense=False,
        return_sparse=True,
        return_colbert_vecs=False,
    )

    # Sparse score ê³„ì‚° (inner product)
    scores = []
    for i, doc_sparse in enumerate(doc_encodings["lexical_weights"]):
        score = 0.0
        for term, query_weight in query_sparse_vector.items():
            if term in doc_sparse:
                score += query_weight * doc_sparse[term]
        scores.append((doc_ids[i], documents[i], score))

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    scores.sort(key=lambda x: x[2], reverse=True)
    return scores[:top_k]


def _reciprocal_rank_fusion(
    dense_results: List[Tuple],
    sparse_results: List[Tuple],
    k: int = 60,
) -> List[str]:
    """
    Reciprocal Rank Fusion

    Args:
        dense_results: Dense ê²€ìƒ‰ ê²°ê³¼ [(doc_id, doc_text, score), ...]
        sparse_results: Sparse ê²€ìƒ‰ ê²°ê³¼ [(doc_id, doc_text, score), ...]
        k: RRF ìƒìˆ˜ (ê¸°ë³¸: 60)

    Returns:
        List[str]: Fusionëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    RRF ê³µì‹:
        score(d) = Î£ 1 / (k + rank_i(d))
    """
    rrf_scores = {}
    doc_texts = {}

    # Dense ìˆœìœ„ ì¶”ê°€
    for rank, (doc_id, doc_text, _) in enumerate(dense_results):
        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1.0 / (k + rank + 1)
        doc_texts[doc_id] = doc_text

    # Sparse ìˆœìœ„ ì¶”ê°€
    for rank, (doc_id, doc_text, _) in enumerate(sparse_results):
        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1.0 / (k + rank + 1)
        doc_texts[doc_id] = doc_text

    # RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    fused_docs = [doc_texts[doc_id] for doc_id in sorted_ids]

    logger.debug(
        f"[RRF] Dense {len(dense_results)} + Sparse {len(sparse_results)} "
        f"â†’ {len(fused_docs)} unique docs"
    )
    return fused_docs


# ========== ë…¸ë“œ 3: Rerank Stage 1 ==========

def rerank_stage1_node(state: RAGState) -> RAGState:
    """
    1ì°¨ Reranking (BGE-reranker-v2-m3)

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: 1ì°¨ reranking ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ì „ëµ:
    - Hybrid search ê²°ê³¼ ì¤‘ ìƒìœ„ 25ê°œë¥¼ reranking
    - ë¹ ë¥¸ ëª¨ë¸ë¡œ ì´ˆê¸° í•„í„°ë§
    """
    logger.info("[Rerank Stage 1] ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    documents = state["documents"]
    metadatas = state["metadatas"]

    if not documents:
        logger.warning("[Rerank Stage 1] ë¬¸ì„œ ì—†ìŒ")
        state["reranked_documents"] = []
        state["reranked_metadatas"] = []
        return add_to_history(state, "rerank_stage1")

    # ìƒìœ„ 25ê°œë§Œ reranking (ì„±ëŠ¥ ìµœì í™”)
    rerank_input_k = min(25, len(documents))
    docs_to_rerank = documents[:rerank_input_k]
    metas_to_rerank = metadatas[:rerank_input_k]

    # Reranking
    reranked_docs = _rerank(
        question, docs_to_rerank, resources.reranker_stage1, rerank_input_k
    )

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘
    doc_to_meta = {doc: meta for doc, meta in zip(docs_to_rerank, metas_to_rerank)}
    reranked_metas = [
        doc_to_meta.get(doc, {"domain": "unknown"}) for doc in reranked_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[Rerank Stage 1] {len(reranked_docs)}ê°œ ë¬¸ì„œ reranking ì™„ë£Œ ({elapsed:.2f}s)"
    )

    state["reranked_documents"] = reranked_docs
    state["reranked_metadatas"] = reranked_metas
    return add_to_history(state, "rerank_stage1")


# ========== ë…¸ë“œ 4: Rerank Stage 2 ==========

def rerank_stage2_node(state: RAGState) -> RAGState:
    """
    2ì°¨ Reranking (BGE-reranker-large)

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: 2ì°¨ reranking ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ì „ëµ:
    - 1ì°¨ reranking ê²°ê³¼ë¥¼ ë” ê°•ë ¥í•œ ëª¨ë¸ë¡œ ì¬í‰ê°€
    - ìµœì¢… top_kê°œ ì„ íƒ (ê¸°ë³¸: 10ê°œ)
    """
    logger.info("[Rerank Stage 2] ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    documents = state["reranked_documents"]
    metadatas = state["reranked_metadatas"]

    if not documents:
        logger.warning("[Rerank Stage 2] ë¬¸ì„œ ì—†ìŒ")
        state["final_documents"] = []
        state["final_metadatas"] = []
        return add_to_history(state, "rerank_stage2")

    # ìµœì¢… top_k ì„ íƒ
    final_k = config.rerank_top_k
    reranked_docs = _rerank(question, documents, resources.reranker_stage2, final_k)

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘
    doc_to_meta = {doc: meta for doc, meta in zip(documents, metadatas)}
    reranked_metas = [
        doc_to_meta.get(doc, {"domain": "unknown"}) for doc in reranked_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[Rerank Stage 2] {len(reranked_docs)}ê°œ ìµœì¢… ë¬¸ì„œ ì„ íƒ ({elapsed:.2f}s)"
    )

    state["final_documents"] = reranked_docs
    state["final_metadatas"] = reranked_metas
    return add_to_history(state, "rerank_stage2")


def _rerank(
    query: str, documents: List[str], reranker: FlagReranker, top_k: int
) -> List[str]:
    """
    ë¬¸ì„œ Reranking

    Args:
        query: ì¿¼ë¦¬
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        reranker: Reranker ëª¨ë¸
        top_k: ìƒìœ„ kê°œ ë°˜í™˜

    Returns:
        List[str]: Rerankingëœ ë¬¸ì„œë“¤
    """
    if not documents:
        return []

    pairs = [[query, doc] for doc in documents]
    scores = reranker.compute_score(pairs, normalize=True)

    # ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬
    if isinstance(scores, (int, float)):
        scores = [scores]

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    scored_docs = list(zip(documents, scores))
    scored_docs.sort(key=lambda x: x[1], reverse=True)

    return [doc for doc, _ in scored_docs[:top_k]]


# ========== ê³„ì† (nodes_part2.pyë¡œ ë¶„í• ) ==========
# ë‹¤ìŒ ë…¸ë“œë“¤:
# - grade_documents_node
# - transform_query_node
# - generate_node
# - hallucination_check_node
# - answer_grading_node
# - web_search_node
# nodes.py ê³„ì† - ë…¸ë“œ 5~10

# ========== ë…¸ë“œ 5: Grade Documents ==========

# ### ìˆ˜ì • ì‹œì‘ ###
def grade_documents_node(state):
    """
    ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ (Corrective RAG)
    with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ í‰ê°€.
    """
    logger.info("[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    question = state["question"]
    documents = state["final_documents"]

    if not documents:
        logger.warning("[GradeDocuments] ë¬¸ì„œ ì—†ìŒ")
        state["document_relevance"] = "not_relevant"
        return add_to_history(state, "grade_documents")

    # with_structured_outputìœ¼ë¡œ DocumentRelevance ëª¨ë¸ ì‚¬ìš©
    structured_llm = resources.langchain_llm_fast.with_structured_output(
        DocumentRelevance,
        method="function_calling",
    )

    system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ê¸°ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- RELEVANT: ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•  ìˆ˜ ìˆëŠ” ì •ë³´ í¬í•¨
- PARTIAL: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ ì¼ë¶€ í¬í•¨
- IRRELEVANT: ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìŒ"""

    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    message_batches = []
    for doc in documents:
        doc_preview = doc[:800] if len(doc) > 800 else doc
        user_prompt = f"ì§ˆë¬¸: {question}\n\në¬¸ì„œ: {doc_preview}"
        message_batches.append([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ])

    # ë³‘ë ¬ ì²˜ë¦¬ (LangChain batch ì‚¬ìš© - 10ê°œ ë™ì‹œ ìš”ì²­)
    try:
        batch_results = structured_llm.batch(message_batches)
        results = []
        for result in batch_results:
            if isinstance(result, DocumentRelevance):
                results.append(result.relevance)
                logger.debug(f"[GradeDocuments] ë¬¸ì„œ í‰ê°€: {result.relevance.value}, ê·¼ê±°: {result.reasoning}")
            else:
                logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…, ê¸°ë³¸ê°’ PARTIAL ì‚¬ìš©")
                results.append(RelevanceType.PARTIAL)
    except Exception as e:
        logger.warning(f"ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {e}, ëª¨ë“  ë¬¸ì„œ PARTIAL ì²˜ë¦¬")
        results = [RelevanceType.PARTIAL] * len(documents)


    # ê²°ê³¼ ì§‘ê³„
    relevant_count = sum(
        1 for r in results if r in (RelevanceType.RELEVANT, RelevanceType.PARTIAL)
    )
    relevance_ratio = relevant_count / len(results)

    if relevance_ratio >= 0.5:
        state["document_relevance"] = "relevant"
        logger.info(
            f"[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„±: RELEVANT ({relevant_count}/{len(results)})"
        )
    else:
        state["document_relevance"] = "not_relevant"
        logger.info(
            f"[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„±: NOT RELEVANT ({relevant_count}/{len(results)})"
        )
        state["web_search_needed"] = True

    elapsed = time.time() - start_time
    logger.info(f"[GradeDocuments] í‰ê°€ ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "grade_documents")
# ### ìˆ˜ì • ì™„ë£Œ ###


# ========== ë…¸ë“œ 6: Transform Query ==========

# ### ìˆ˜ì • ì‹œì‘ ###
def transform_query_node(state):
    """
    ì¿¼ë¦¬ ì¬ì‘ì„± (Query Transformation)
    with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±.
    """
    logger.info("[TransformQuery] ì¿¼ë¦¬ ì¬ì‘ì„± ì‹œì‘")
    start_time = time.time()

    _increment_retry_count(state)

    resources = get_resources()
    question = state["question"]

    system_prompt = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ë” ì í•©í•œ í˜•íƒœë¡œ ì¬ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

íŒë‹¨ ê¸°ì¤€:
- PRESERVE: ì§ˆë¬¸ì´ ì´ë¯¸ ì¶©ë¶„íˆ êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ì— ì í•©í•¨
- REWRITE: ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì¬ì‘ì„± í•„ìš”

ì¬ì‘ì„± ì§€ì¹¨ (REWRITEì¸ ê²½ìš°):
- í•µì‹¬ í‚¤ì›Œë“œ ê°•ì¡°
- êµ¬ì²´ì ì¸ ìš©ì–´ ì‚¬ìš©
- ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""

    user_prompt = f"ì›ë³¸ ì§ˆë¬¸: {question}"

    try:
        structured_llm = resources.langchain_llm_fast.with_structured_output(
            RewrittenQuery,
            method="function_calling",
        )
        result: RewrittenQuery = structured_llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ])

        if result.action == QueryRewriteAction.REWRITE and result.rewritten_query.strip():
            transformed = result.rewritten_query.strip()
            logger.info(f"[TransformQuery] ì›ë³¸: {question}")
            logger.info(f"[TransformQuery] ì¬ì‘ì„±: {transformed}")
            logger.info(f"[TransformQuery] ê·¼ê±°: {result.reasoning}")
            state["transformed_query"] = transformed
            state["question"] = transformed
        else:
            logger.info(f"[TransformQuery] ì›ë³¸ ìœ ì§€: {question}")
            logger.info(f"[TransformQuery] ê·¼ê±°: {result.reasoning}")
            state["transformed_query"] = question

    except Exception as e:
        logger.error(f"[TransformQuery] ì‹¤íŒ¨: {e}, ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€")
        state["transformed_query"] = question

    elapsed = time.time() - start_time
    logger.info(f"[TransformQuery] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "transform_query")
# ### ìˆ˜ì • ì™„ë£Œ ###


# ========== ë…¸ë“œ 7: Generate ==========

# ### ìˆ˜ì • ì‹œì‘ ###
def generate_node(state):
    """
    ë‹µë³€ ìƒì„± (LLM)
    bind_toolsë¥¼ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±.
    LLMì´ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ web_search ë„êµ¬ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŒ.
    """
    logger.info("[Generate] ë‹µë³€ ìƒì„± ì‹œì‘")
    start_time = time.time()

    resources = get_resources()

    question = state["question"]
    documents = state["final_documents"]
    metadatas = state["final_metadatas"]

    if not documents:
        logger.warning("[Generate] ë¬¸ì„œ ì—†ìŒ")
        state["generation"] = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•´ë³´ì‹œê² ì–´ìš”?"
        return add_to_history(state, "generate")

    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context_block = "\n\n".join(
        f"[ë¬¸ì„œ {i+1}] {meta.get('domain', 'unknown')}\n{doc}"
        for i, (doc, meta) in enumerate(zip(documents, metadatas))
    )

    # ë‹µë³€ í˜•ì‹ ê°€ì´ë“œ ì¶”ê°€
    format_guide = """

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš¨ğŸš¨ğŸš¨ ABSOLUTE MANDATORY ë‹µë³€ í˜•ì‹ ê·œì¹™ ğŸš¨ğŸš¨ğŸš¨
í”„ë¡ íŠ¸ì—”ë“œëŠ” ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ SIMPLE OUTPUT TEMPLATE (ì„¹ì…˜ ë¼ë²¨ ë¶ˆí•„ìš”):

[ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª… 1-3ë¬¸ì¥]

- bullet point 1
- bullet point 2
- bullet point 3

\```ì–¸ì–´
ì½”ë“œ
\```

ğŸ”´ ì¤‘ìš”: "ì˜ˆì‹œ:", "ì£¼ìš” íŠ¹ì§•:", "ì°¨ì´ì :" ê°™ì€ ì„¹ì…˜ ë¼ë²¨ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!
ì½”ë“œ ë¸”ë¡ì€ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìë™ ë Œë”ë§ë©ë‹ˆë‹¤.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… ì •ë‹µ (ì´ í˜•ì‹ ê·¸ëŒ€ë¡œ ë³µì œí•˜ì„¸ìš”):
\"\"\"
ì»´í”„ë¦¬í—¨ì…˜ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œ ì¤„ë¡œ ìƒì„±í•˜ëŠ” ë¬¸ë²•ì´ì—ìš”. forë¬¸ê³¼ ifë¬¸ì„ ì¡°í•©í•´ ê°„ê²°í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì£ .

- ëŒ€ê´„í˜¸ [ ] ì•ˆì— í‘œí˜„ì‹ê³¼ forë¬¸ ì¡°í•©
- ì¡°ê±´ë¬¸(if)ìœ¼ë¡œ í•„í„°ë§ ê°€ëŠ¥
- ì½”ë“œê°€ ì§§ê³  ì½ê¸° ì‰¬ì›€

\```python
squares = [x**2 for x in range(5)]
evens = [x for x in range(10) if x % 2 == 0]
\```
\"\"\"

âœ… ì •ë‹µ (ë¹„êµ ì„¤ëª…):
\"\"\"
ê·¸ë¦¬ë””ì™€ DPëŠ” ìµœì í™” ë¬¸ì œ í•´ê²° ë°©ë²•ì´ì—ìš”. ê·¸ë¦¬ë””ëŠ” ë§¤ ìˆœê°„ ìµœì„ ì˜ ì„ íƒì„ í•˜ê³ , DPëŠ” ëª¨ë“  ê²½ìš°ë¥¼ ê³ ë ¤í•´ìš”.

- ê·¸ë¦¬ë””: ë¹ ë¥´ì§€ë§Œ ìµœì í•´ ë³´ì¥ ì•ˆ ë¨
- DP: ëŠë¦¬ì§€ë§Œ ìµœì í•´ ë³´ì¥

\```python
# ê·¸ë¦¬ë””
coins = [500, 100, 50, 10]
count = sum(n // c for c in coins)

# DP
memo = {}
def min_coins(n):
    if n in memo: return memo[n]
    # ... DP ë¡œì§
\```
\"\"\"

âŒ ì ˆëŒ€ ê¸ˆì§€ (ë¼ë²¨ ì‚¬ìš©):
\"\"\"
ì»´í”„ë¦¬í—¨ì…˜ì€ ë¬¸ë²•ì´ì—ìš”.

ì£¼ìš” íŠ¹ì§•: << ì„¹ì…˜ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€!
- íŠ¹ì§• 1
- íŠ¹ì§• 2

ì˜ˆì‹œ: << ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€!
\```python
code
\```
\"\"\"

âŒ ì ˆëŒ€ ê¸ˆì§€ (bulletë§Œ ìˆê³  ì„¤ëª… ì—†ìŒ):
\"\"\"
ì»´í”„ë¦¬í—¨ì…˜:
- ë¦¬ìŠ¤íŠ¸ ìƒì„±
- í•œ ì¤„ë¡œ ì‘ì„±

\```python
code
\```
\"\"\"

ğŸ”´ğŸ”´ğŸ”´ í•µì‹¬ ê·œì¹™ 3ê°€ì§€ ğŸ”´ğŸ”´ğŸ”´
1. ì‹œì‘ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ 1-3ê°œ (bullet ê¸ˆì§€)
2. bullet pointsì™€ ì½”ë“œ ë¸”ë¡ ì‚¬ì´ì— ë¹ˆ ì¤„ 1ê°œ
3. "ì˜ˆì‹œ:", "ì£¼ìš” íŠ¹ì§•:" ê°™ì€ ì„¹ì…˜ ë¼ë²¨ ì ˆëŒ€ ê¸ˆì§€ (ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ìœ¼ë¡œ ì¶©ë¶„)

ìœ„ ê·œì¹™ì„ ì–´ê¸°ë©´ ë‹µë³€ì´ ì¦‰ì‹œ ê±°ë¶€ë©ë‹ˆë‹¤!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

    system_content = resources.system_prompt + format_guide
    user_content = f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{context_block}"

    messages = [
        SystemMessage(content=system_content),
        HumanMessage(content=user_content),
    ]

    try:
        # bind_toolsë¡œ ë„êµ¬ ë°”ì¸ë”© (LLMì´ í•„ìš”ì‹œ web_search í˜¸ì¶œ ê°€ëŠ¥)
        rag_tools = get_rag_tools()
        llm_with_tools = resources.langchain_llm.bind_tools(
            rag_tools,
            tool_choice="auto",  # LLMì´ ìë™ìœ¼ë¡œ ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ ê²°ì •
        )

        response = llm_with_tools.invoke(messages)

        # tool_callsê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
        if response.tool_calls:
            logger.info(f"[Generate] Tool calls ê°ì§€: {[tc['name'] for tc in response.tool_calls]}")

            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]

                if tool_name == "web_search":
                    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                    from .tools import web_search as web_search_tool_func
                    search_result = web_search_tool_func.invoke(tool_args)
                    logger.info(f"[Generate] ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {tool_args.get('query', '')}")

                    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹¤ì‹œ ë‹µë³€ ìƒì„±
                    enhanced_context = f"{context_block}\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼]\n{search_result}"
                    enhanced_messages = [
                        SystemMessage(content=system_content),
                        HumanMessage(content=f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{enhanced_context}"),
                    ]

                    # ë„êµ¬ ì—†ì´ ìµœì¢… ë‹µë³€ ìƒì„±
                    final_response = resources.langchain_llm.invoke(enhanced_messages)
                    answer_text = final_response.content

                elif tool_name == "answer_directly":
                    # ë°”ë¡œ ë‹µë³€ (tool ì—†ì´ ì¬í˜¸ì¶œ)
                    final_response = resources.langchain_llm.invoke(messages)
                    answer_text = final_response.content
                else:
                    answer_text = response.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # tool_callsê°€ ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€
            answer_text = response.content

        # ê¸°ì¡´ ì¶œì²˜ ì œê±° ë° íˆ´ëª… ì •ë¦¬
        answer_text = _clean_tool_mentions(_strip_existing_sources(answer_text))

        # URL ì¶œì²˜ ì¶”ê°€ (tavily.com ë“± ê²€ìƒ‰ ì—”ì§„ URL ì œì™¸)
        source_urls = []
        excluded_domains = ["tavily.com", "tavily", "search.tavily.com"]

        for meta in metadatas:
            url = meta.get("url", "unknown")
            # unknownì´ ì•„ë‹ˆê³ , ì¤‘ë³µë˜ì§€ ì•Šê³ , ì œì™¸ ë„ë©”ì¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            if url != "unknown" and url not in source_urls:
                # ì œì™¸ ë„ë©”ì¸ ì²´í¬
                if not any(domain in url.lower() for domain in excluded_domains):
                    source_urls.append(url)

        if source_urls:
            sources_section = "\n\nğŸ“š ì°¸ê³ :\n" + "\n".join(
                f"- {url}" for url in source_urls
            )
            answer = answer_text + sources_section
        else:
            answer = answer_text

        state["generation"] = answer
        logger.info("[Generate] ë‹µë³€ ìƒì„± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"[Generate] ì‹¤íŒ¨: {e}")
        state["generation"] = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    elapsed = time.time() - start_time
    logger.info(f"[Generate] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "generate")
# ### ìˆ˜ì • ì™„ë£Œ ###


def _strip_existing_sources(answer_text: str) -> str:
    """ê¸°ì¡´ ì¶œì²˜ ì„¹ì…˜ ì œê±°"""
    marker = "ğŸ“š ì°¸ê³ "
    if marker in answer_text:
        return answer_text.split(marker)[0].rstrip()
    return answer_text


def _clean_tool_mentions(answer_text: str) -> str:
    """
    ë³¸ë¬¸ì—ì„œ tavily/websearch ë“± íˆ´ ì´ë¦„ì„ ì œê±°í•´ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“ ë‹¤.
    """
    cleaned = answer_text
    for token in ["tavily", "websearch", "web search", "Tavily", "WebSearch"]:
        cleaned = re.sub(rf"\(?\b{re.escape(token)}\b\)?", "", cleaned, flags=re.IGNORECASE)
    # Collapse double spaces left by removals
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    return cleaned.strip()


# ========== ë…¸ë“œ 8: Hallucination Check ==========

# ### ìˆ˜ì • ì‹œì‘ ###
def hallucination_check_node(state):
    """
    í™˜ê° ê²€ì¦ (Self-RAG)
    with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ ê²€ì¦.
    """
    logger.info("[HallucinationCheck] í™˜ê° ê²€ì¦ ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    generation = state["generation"]
    documents = state["final_documents"]

    if not documents:
        logger.warning("[HallucinationCheck] ë¬¸ì„œ ì—†ìŒ, ê²€ì¦ ìŠ¤í‚µ")
        state["hallucination_grade"] = "not_sure"
        return add_to_history(state, "hallucination_check")

    # ì¶œì²˜ ì œê±°í•œ ë‹µë³€ë§Œ ê²€ì¦
    answer_only = _clean_tool_mentions(_strip_existing_sources(generation))

    # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ truncate)
    context_preview = "\n\n".join(documents[:3])
    if len(context_preview) > 2000:
        context_preview = context_preview[:2000] + "..."

    system_prompt = """ë‹¹ì‹ ì€ í™˜ê°(hallucination) ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

íŒë‹¨ ê¸°ì¤€:
- SUPPORTED: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ë¬¸ì„œì— ê·¼ê±°í•¨
- NOT_SUPPORTED: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë¨ (í™˜ê°)
- NOT_SURE: íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€"""

    user_prompt = f"""ë‹µë³€:
{answer_only}

ì œê³µëœ ë¬¸ì„œ:
{context_preview}

ë‹µë³€ì˜ ëª¨ë“  ì£¼ì¥ì´ ë¬¸ì„œì—ì„œ í™•ì¸ë©ë‹ˆê¹Œ?"""

    try:
        structured_llm = resources.langchain_llm_fast.with_structured_output(
            HallucinationGrade,
            method="function_calling",
        )
        result: HallucinationGrade = structured_llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ])

        state["hallucination_grade"] = result.grade.value
        logger.info(f"[HallucinationCheck] ê²°ê³¼: {result.grade.value}, ê·¼ê±°: {result.reasoning}")

        if result.grade == HallucinationType.NOT_SUPPORTED:
            state["web_search_needed"] = True

    except Exception as e:
        logger.error(f"[HallucinationCheck] ì‹¤íŒ¨: {e}")
        state["hallucination_grade"] = "not_sure"

    elapsed = time.time() - start_time
    logger.info(f"[HallucinationCheck] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "hallucination_check")
# ### ìˆ˜ì • ì™„ë£Œ ###


# ========== ë…¸ë“œ 9: Answer Grading ==========

# ### ìˆ˜ì • ì‹œì‘ ###
def answer_grading_node(state):
    """
    ë‹µë³€ í’ˆì§ˆ í‰ê°€ (Self-RAG)
    with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ tool calling ë°©ì‹ìœ¼ë¡œ í‰ê°€.
    """
    logger.info("[AnswerGrading] ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
    start_time = time.time()

    resources = get_resources()
    question = state["question"]
    generation = state["generation"]

    # ì¶œì²˜ ì œê±°í•œ ë‹µë³€ë§Œ í‰ê°€
    answer_only = _clean_tool_mentions(_strip_existing_sources(generation))

    system_prompt = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- USEFUL: ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í•¨
- NOT_USEFUL: ì§ˆë¬¸ì— ë‹µë³€í•˜ì§€ ëª»í•¨"""

    user_prompt = f"""ì§ˆë¬¸: {question}

ë‹µë³€: {answer_only}

ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í•©ë‹ˆê¹Œ?"""

    try:
        structured_llm = resources.langchain_llm_fast.with_structured_output(
            UsefulnessGrade,
            method="function_calling",
        )
        result: UsefulnessGrade = structured_llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ])

        state["answer_usefulness"] = result.grade.value
        logger.info(f"[AnswerGrading] ê²°ê³¼: {result.grade.value}, ê·¼ê±°: {result.reasoning}")

        if result.grade == UsefulnessType.NOT_USEFUL:
            state["web_search_needed"] = True

    except Exception as e:
        logger.error(f"[AnswerGrading] ì‹¤íŒ¨: {e}")
        state["answer_usefulness"] = "useful"  # ì‹¤íŒ¨ ì‹œ ê¸ì •ìœ¼ë¡œ ê°€ì •

    elapsed = time.time() - start_time
    logger.info(f"[AnswerGrading] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "answer_grading")
# ### ìˆ˜ì • ì™„ë£Œ ###


# ========== ë…¸ë“œ 10: Web Search ==========

def web_search_node(state):
    """ì›¹ ê²€ìƒ‰ fallback (Corrective RAG)"""
    logger.info("[WebSearch] ì›¹ ê²€ìƒ‰ ì‹œì‘")
    start_time = time.time()

    _increment_retry_count(state)

    web_search_tool = get_web_search_tool()
    question = state["question"]

    if not web_search_tool.enabled:
        logger.warning("[WebSearch] ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”ë¨")
        state["final_documents"] = []
        state["final_metadatas"] = []
        return add_to_history(state, "web_search")

    # ì›¹ ê²€ìƒ‰ ì‹¤í–‰
    documents, metadatas = web_search_tool.search_with_metadata(question)

    state["final_documents"] = documents
    state["final_metadatas"] = metadatas

    elapsed = time.time() - start_time
    logger.info(f"[WebSearch] {len(documents)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "web_search")


# NEW START - ê°œì¸í™” ë…¸ë“œ

# ========== ë…¸ë“œ 11: Load User Context (ê°œì¸í™”) ==========

def load_user_context_node(state: RAGState) -> RAGState:
    """
    ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ê°œì¸í™”)

    DBì—ì„œ ì‚¬ìš©ìì˜ ê³¼ê±° ì„œë¹„ìŠ¤ ì„ íƒ ì´ë ¥ì„ ì¡°íšŒí•˜ê³ ,
    í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„ íƒ í•­ëª© ë° "ìŠì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ”" í•­ëª©ì„ ì‹ë³„í•©ë‹ˆë‹¤.

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ìƒíƒœ
    """
    logger.info("[LoadUserContext] ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹œì‘")
    start_time = time.time()

    user_id = state.get("user_id", "")
    question = state["question"]

    if not user_id:
        logger.info("[LoadUserContext] user_id ì—†ìŒ, ê°œì¸í™” ìŠ¤í‚µ")
        return add_to_history(state, "load_user_context")

    try:
        # Step 1: DBì—ì„œ ì‚¬ìš©ì ì„ íƒ ì´ë ¥ ì¡°íšŒ
        # TODO: ì‹¤ì œ DB ì—°ë™ êµ¬í˜„ í•„ìš” (í˜„ì¬ëŠ” mock)
        user_selections = _fetch_user_selections_from_db(user_id)

        if not user_selections:
            logger.info(f"[LoadUserContext] ì‚¬ìš©ì {user_id}ì˜ ì„ íƒ ì´ë ¥ ì—†ìŒ")
            return add_to_history(state, "load_user_context")

        # Step 2: ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = _extract_keywords(question)

        # Step 3: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„ íƒ í•­ëª© í•„í„°ë§
        related_selections = []
        for selection in user_selections:
            if _has_relevance(selection, question_keywords):
                related_selections.append(selection)

        # Step 4: "ìŠì—ˆì„ ê°€ëŠ¥ì„±" íŒë‹¨
        # ì„ íƒí–ˆì§€ë§Œ ì§ˆë¬¸ì—ì„œ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ì•Šì€ í•­ëª© = ìƒê¸° í›„ë³´
        question_lower = question.lower()
        forgotten_candidates = []
        for selection in related_selections:
            service_name = selection.get("service_name", "").lower()
            selected_option = selection.get("selected_option", "").lower()

            # ì„œë¹„ìŠ¤ëª…ì´ë‚˜ ì„ íƒ ì˜µì…˜ì´ ì§ˆë¬¸ì— ì—†ìœ¼ë©´ ìŠì—ˆì„ ê°€ëŠ¥ì„±
            if service_name not in question_lower and selected_option not in question_lower:
                forgotten_candidates.append(selection)

        state["user_selections"] = user_selections
        state["related_selections"] = related_selections
        state["forgotten_candidates"] = forgotten_candidates

        logger.info(
            f"[LoadUserContext] ë¡œë“œ ì™„ë£Œ - "
            f"ì „ì²´: {len(user_selections)}, "
            f"ê´€ë ¨: {len(related_selections)}, "
            f"ìƒê¸° í›„ë³´: {len(forgotten_candidates)}"
        )

    except Exception as e:
        logger.error(f"[LoadUserContext] ì‹¤íŒ¨: {e}")
        state["user_selections"] = []
        state["related_selections"] = []
        state["forgotten_candidates"] = []

    elapsed = time.time() - start_time
    logger.info(f"[LoadUserContext] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "load_user_context")


def _fetch_user_selections_from_db(user_id: str) -> List[Dict]:
    """
    DBì—ì„œ ì‚¬ìš©ì ì„ íƒ ì´ë ¥ ì¡°íšŒ (Mock êµ¬í˜„)

    TODO: ì‹¤ì œ DB ì—°ë™ìœ¼ë¡œ êµì²´ í•„ìš”

    Returns:
        List[Dict]: ì‚¬ìš©ì ì„ íƒ ì´ë ¥
            - service_name: ì„œë¹„ìŠ¤ëª…
            - selected_option: ì„ íƒí•œ ì˜µì…˜
            - category: ì¹´í…Œê³ ë¦¬
            - selected_at: ì„ íƒ ì¼ì‹œ
    """
    # Mock ë°ì´í„° - ì‹¤ì œ êµ¬í˜„ ì‹œ DB ì¿¼ë¦¬ë¡œ êµì²´
    # ì˜ˆ: SELECT * FROM user_selections WHERE user_id = ?
    logger.debug(f"[DB Mock] ì‚¬ìš©ì {user_id} ì„ íƒ ì´ë ¥ ì¡°íšŒ")

    # ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ:
    # from .database import get_db_connection
    # db = get_db_connection()
    # return db.query("SELECT * FROM user_selections WHERE user_id = ?", [user_id])

    return []  # ì‹¤ì œ DB ì—°ë™ ì „ê¹Œì§€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


def _extract_keywords(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸

    Returns:
        List[str]: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡
    """
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ + ë¶ˆìš©ì–´ ì œê±°)
    # TODO: ë” ì •êµí•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ë“±)
    stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì™€", "ê³¼", "í•˜ê³ ", "ìˆ", "ì—†", "ìˆ˜"}

    words = text.lower().replace("?", "").replace(".", "").split()
    keywords = [w for w in words if len(w) > 1 and w not in stopwords]

    return keywords


def _has_relevance(selection: Dict, keywords: List[str]) -> bool:
    """
    ì„ íƒ í•­ëª©ì´ í‚¤ì›Œë“œì™€ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨

    Args:
        selection: ì‚¬ìš©ì ì„ íƒ í•­ëª©
        keywords: ì§ˆë¬¸ í‚¤ì›Œë“œ ëª©ë¡

    Returns:
        bool: ê´€ë ¨ ì—¬ë¶€
    """
    service_name = selection.get("service_name", "").lower()
    category = selection.get("category", "").lower()
    selected_option = selection.get("selected_option", "").lower()

    selection_text = f"{service_name} {category} {selected_option}"

    # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ê´€ë ¨ ìˆìŒ
    for keyword in keywords:
        if keyword in selection_text:
            return True

    return False


# ========== ë…¸ë“œ 12: Personalize Response (ê°œì¸í™”) ==========

def personalize_response_node(state: RAGState) -> RAGState:
    """
    ë‹µë³€ ê°œì¸í™” (ìƒê¸° ë©”ì‹œì§€ ì£¼ì…)

    ìƒì„±ëœ ë‹µë³€ì— ì‚¬ìš©ìê°€ ìŠì—ˆì„ ìˆ˜ ìˆëŠ” ê³¼ê±° ì„ íƒ ì‚¬í•­ì„ ìƒê¸°ì‹œí‚¤ëŠ”
    ë©”ì‹œì§€ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: ê°œì¸í™”ëœ ë‹µë³€ì´ í¬í•¨ëœ ìƒíƒœ
    """
    logger.info("[PersonalizeResponse] ë‹µë³€ ê°œì¸í™” ì‹œì‘")
    start_time = time.time()

    generation = state["generation"]
    forgotten_candidates = state.get("forgotten_candidates", [])

    if not forgotten_candidates:
        logger.info("[PersonalizeResponse] ìƒê¸°í•  ë‚´ìš© ì—†ìŒ, ìŠ¤í‚µ")
        state["reminder_added"] = False
        return add_to_history(state, "personalize_response")

    try:
        # ìƒê¸° ë©”ì‹œì§€ ìƒì„± (ìµœëŒ€ 2ê°œ í•­ëª©ë§Œ)
        reminder_items = forgotten_candidates[:2]
        reminder_parts = []

        for item in reminder_items:
            service_name = item.get("service_name", "")
            selected_option = item.get("selected_option", "")

            if service_name and selected_option:
                reminder_parts.append(f"'{service_name}'ì—ì„œ '{selected_option}'")
            elif service_name:
                reminder_parts.append(f"'{service_name}'")

        if reminder_parts:
            # ìì—°ìŠ¤ëŸ¬ìš´ ìƒê¸° ë©”ì‹œì§€ êµ¬ì„±
            if len(reminder_parts) == 1:
                items_text = reminder_parts[0]
            else:
                items_text = f"{reminder_parts[0]}ê³¼(ì™€) {reminder_parts[1]}"

            reminder_message = (
                f"\n\nğŸ’¡ **ì°¸ê³ **: ê³ ê°ë‹˜ê»˜ì„œëŠ” ì´ì „ì— {items_text}ì„(ë¥¼) "
                f"ë³´ì…¨ëŠ”ë°ìš”, ì´ ë¶€ë¶„ë„ í•¨ê»˜ í™•ì¸í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

            # ì¶œì²˜ ì„¹ì…˜ ì•ì— ì‚½ì…
            if "ğŸ“š ì°¸ê³ :" in generation:
                parts = generation.split("ğŸ“š ì°¸ê³ :")
                personalized_generation = parts[0].rstrip() + reminder_message + "\n\nğŸ“š ì°¸ê³ :" + parts[1]
            else:
                personalized_generation = generation + reminder_message

            state["generation"] = personalized_generation
            state["reminder_added"] = True

            logger.info(f"[PersonalizeResponse] ìƒê¸° ë©”ì‹œì§€ ì¶”ê°€: {items_text}")
        else:
            state["reminder_added"] = False

    except Exception as e:
        logger.error(f"[PersonalizeResponse] ì‹¤íŒ¨: {e}")
        state["reminder_added"] = False

    elapsed = time.time() - start_time
    logger.info(f"[PersonalizeResponse] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "personalize_response")

# NEW END - ê°œì¸í™” ë…¸ë“œ


# ========== ë…¸ë“œ 13: Suggest Related Questions + Reminder (ë¹„ë™ê¸° ê°œì¸í™” í†µí•©) ==========

def suggest_related_questions_node(state: RAGState) -> RAGState:
    """
    ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ + ìƒê¸° ë©”ì‹œì§€ ìƒì„± (ë¹„ë™ê¸° ê°œì¸í™” í†µí•©)

    1. ì‚¬ìš©ìê°€ ìŠì—ˆì„ ìˆ˜ ìˆëŠ” ê³¼ê±° ì„ íƒ ì‚¬í•­ ìƒê¸° ë©”ì‹œì§€ ìƒì„±
    2. í˜„ì¬ ì§ˆë¬¸ê³¼ ìƒì„±ëœ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ ì§ˆë¬¸ 3ê°œ ì¶”ì²œ

    Args:
        state (RAGState): í˜„ì¬ ìƒíƒœ

    Returns:
        RAGState: ê´€ë ¨ ì§ˆë¬¸ ë° ìƒê¸° ë©”ì‹œì§€ê°€ ì¶”ê°€ëœ ìƒíƒœ
    """
    logger.info("[SuggestQuestions] ë¹„ë™ê¸° ê°œì¸í™” ì‹œì‘ (ì§ˆë¬¸ ì¶”ì²œ + ìƒê¸° ë©”ì‹œì§€)")
    start_time = time.time()

    # ========== Part 1: ìƒê¸° ë©”ì‹œì§€ ìƒì„± (ë¹ ë¥¸ ë¬¸ìì—´ ì¡°ì‘) ==========
    forgotten_candidates = state.get("forgotten_candidates", [])
    reminder_message = ""

    if forgotten_candidates:
        logger.info(f"[SuggestQuestions] ìƒê¸° ë©”ì‹œì§€ ìƒì„± ì¤‘ ({len(forgotten_candidates)}ê°œ í›„ë³´)")
        try:
            # ìƒê¸° ë©”ì‹œì§€ ìƒì„± (ìµœëŒ€ 2ê°œ í•­ëª©ë§Œ)
            reminder_items = forgotten_candidates[:2]
            reminder_parts = []

            for item in reminder_items:
                service_name = item.get("service_name", "")
                selected_option = item.get("selected_option", "")

                if service_name and selected_option:
                    reminder_parts.append(f"'{service_name}'ì—ì„œ '{selected_option}'")
                elif service_name:
                    reminder_parts.append(f"'{service_name}'")

            if reminder_parts:
                # ìì—°ìŠ¤ëŸ¬ìš´ ìƒê¸° ë©”ì‹œì§€ êµ¬ì„±
                if len(reminder_parts) == 1:
                    items_text = reminder_parts[0]
                else:
                    items_text = f"{reminder_parts[0]}ê³¼(ì™€) {reminder_parts[1]}"

                reminder_message = (
                    f"ğŸ’¡ ê³ ê°ë‹˜ê»˜ì„œëŠ” ì´ì „ì— {items_text}ì„(ë¥¼) "
                    f"ë³´ì…¨ëŠ”ë°ìš”, ì´ ë¶€ë¶„ë„ í•¨ê»˜ í™•ì¸í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

                logger.info(f"[SuggestQuestions] ìƒê¸° ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {items_text}")
        except Exception as e:
            logger.error(f"[SuggestQuestions] ìƒê¸° ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")

    state["reminder_message"] = reminder_message

    # ========== Part 2: ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ (LLM í˜¸ì¶œ) ==========
    resources = get_resources()

    question = state["question"]
    answer = state["generation"]
    user_context = state.get("user_context", {})

    # user_contextì—ì„œ í•™ìŠµ ëª©í‘œì™€ ê´€ì‹¬ ì£¼ì œ ì¶”ì¶œ
    learning_goals = user_context.get("learning_goals", "")
    interested_topics = user_context.get("interested_topics", "")

    # ì¶œì²˜ ì œê±°í•œ ë‹µë³€ë§Œ ì‚¬ìš©
    answer_only = _clean_tool_mentions(_strip_existing_sources(answer))

    # ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = ""
    if learning_goals or interested_topics:
        context_text = "\n\nì‚¬ìš©ì í”„ë¡œí•„:"
        if learning_goals:
            context_text += f"\n- í•™ìŠµ ëª©í‘œ: {learning_goals}"
        if interested_topics:
            context_text += f"\n- ê´€ì‹¬ ì£¼ì œ: {interested_topics}"

    system_prompt = """ë‹¹ì‹ ì€ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³´ê³ ,
ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”.

ì¶”ì²œ ê¸°ì¤€:
- í˜„ì¬ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ì‹¬í™” ì§ˆë¬¸
- í•™ìŠµ ë‹¨ê³„ë¥¼ ê³ ë ¤í•œ ì ì ˆí•œ ë‚œì´ë„
- ì‹¤ë¬´ì—ì„œ ìì£¼ ë§ˆì£¼ì¹˜ëŠ” ìƒí™©
- ì‚¬ìš©ìì˜ í•™ìŠµ ëª©í‘œì™€ ê´€ì‹¬ì‚¬ ë°˜ì˜

í˜•ì‹: ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ, êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
ì˜ˆì‹œ:
- Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?
- git mergeì™€ git rebaseì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
- ë”•ì…”ë„ˆë¦¬ì—ì„œ íŠ¹ì • í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•ì€?"""

    user_prompt = f"""í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ìš”ì•½: {answer_only[:500]}...{context_text}

ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ë¬¼ì–´ë³¼ ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”."""

    try:
        response = resources.llm_client.chat.completions.create(
            model=get_config().context_quality_model,  # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì•½ê°„ ë†’ê²Œ
            max_tokens=200,
        )

        suggestions_text = response.choices[0].message.content.strip()

        # íŒŒì‹±: ê° ì¤„ì„ ì§ˆë¬¸ìœ¼ë¡œ ì¶”ì¶œ (- ë‚˜ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¤„)
        import re
        questions = []
        for line in suggestions_text.split("\n"):
            line = line.strip()
            # - ë‚˜ 1. 2. ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì¶”ì¶œ
            match = re.match(r'^[-â€¢*\d.)\]]+\s*(.+)$', line)
            if match:
                question_text = match.group(1).strip()
                if question_text and len(question_text) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                    questions.append(question_text)

        # ìµœëŒ€ 3ê°œë§Œ
        related_questions = questions[:3]

        state["related_questions"] = related_questions

        logger.info(f"[SuggestQuestions] {len(related_questions)}ê°œ ì§ˆë¬¸ ì¶”ì²œ ì™„ë£Œ")
        for i, q in enumerate(related_questions, 1):
            logger.info(f"  {i}. {q[:50]}...")

    except Exception as e:
        logger.error(f"[SuggestQuestions] ì‹¤íŒ¨: {e}")
        state["related_questions"] = []

    elapsed = time.time() - start_time
    logger.info(f"[SuggestQuestions] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "suggest_related_questions")
